{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KJTesting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G-e4qxrOG1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "#everything below is defining activation functions\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "#def relu(input):\n",
        "  #/return max((0, max(input)))\n",
        "\n",
        "def d_relu(input):\n",
        "  if(input < 0 or input == 0):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def d_sigmoid(input):\n",
        "  return sigmoid(input) * (1 - sigmoid(input))\n",
        "\n",
        "def tanh(input):\n",
        "  top = (math.exp(input) - math.exp(-input))\n",
        "  bottom = (math.exp(input) + math.exp(-input))\n",
        "  return (top/bottom)\n",
        "\n",
        "#helper functions for tanh\n",
        "def cosh(input):\n",
        "  return ((math.exp(input) + math.exp(-input)) / 2)\n",
        "def sinh(input):\n",
        "  return ((math.exp(input) - math.exp(-input)) / 2) \n",
        "\n",
        "def d_tanh(input):\n",
        "  top = (math.pow(cosh(input), 2) - math.pow(sinh(input), 2))\n",
        "  bottom = math.pow(input, 2)\n",
        "  return (top / bottom)\n",
        "\n",
        "def softmax(z):\n",
        "  # subracting the max adds numerical stability\n",
        "  shiftx = z - np.max(z,axis=1)[:,np.newaxis]\n",
        "  exps = np.exp(shiftx)\n",
        "  return exps / np.sum(exps,axis=1)[:,np.newaxis]\n",
        "\n",
        "def d_softmax(Y_hat, Y):\n",
        "  return Y_hat - Y\n",
        "\n",
        "def linear(input):\n",
        "  return input\n",
        "def d_linear(input):\n",
        "  return 1\n",
        "\n",
        "\n",
        "\n",
        "def d_activate(input_X, activation_function):\n",
        "  if(activation_function.lower() == 'relu'):\n",
        "    return d_relu(input_X)\n",
        "  elif(activation_function.lower() == 'sigmoid'):\n",
        "    return d_sigmoid(input_X)\n",
        "  elif(activation_function.lower() == 'tanh'):\n",
        "    return d_tanh(input_X)\n",
        "  elif(activation_function.lower() == 'softmax'):\n",
        "    return d_softmax(input_X)\n",
        "  elif(activation_function.lower() == 'linear'):\n",
        "    return d_linear(input_X)\n",
        "  else:\n",
        "    print(\"That activation function is not defined!\")\n",
        "    return None\n",
        "\n",
        "def activate(input_X, activation_function):\n",
        "  if(activation_function.lower() == 'relu'):\n",
        "    return relu(input_X)\n",
        "  elif(activation_function.lower() == 'sigmoid'):\n",
        "    return sigmoid(input_X)\n",
        "  elif(activation_function.lower() == 'tanh'):\n",
        "    return tanh(input_X)\n",
        "  elif(activation_function.lower() == 'softmax'):\n",
        "    return softmax(input_X)\n",
        "  elif(activation_function.lower() == 'linear'):\n",
        "    return linear(input_X)\n",
        "  else:\n",
        "    print(\"That activation function is not defined!\")\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alz211mvN7jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self, batch, lr):\n",
        "    #initializing self lists to keep track of stuff for bacthes, forward prop & backporp\n",
        "    self.batch = batch\n",
        "    self.lr = lr\n",
        "    self.W = []\n",
        "    self.B = []\n",
        "    self.A = []\n",
        "    self.Z = []\n",
        "    self.X = []\n",
        "    self.layers = 0\n",
        "    self.tempW = []\n",
        "    self.tempB = []\n",
        "    self.initialized = False\n",
        "\n",
        "    #store error for backprop\n",
        "    self.output_error = []\n",
        "  \n",
        "  #initialize the weights during 'model.add' so we can test our network shapes dynamically w/out model.compile\n",
        "  #added an output bool here so we can make sure the shape of the output network is (1,n)\n",
        "  def initial_weights(self, input_data, output_shape):\n",
        "    B = np.zeros((1, output_shape))\n",
        "    #assigning the shape \n",
        "    W = np.random.uniform(-1e-3, 1e-3, size = (input_data.shape[len(input_data.shape) - 1], output_shape))\n",
        "    self.B.append(B)\n",
        "    self.W.append(W)\n",
        "\n",
        "  def add(self, input_data, output_shape, activation, layer_number, final_layer):\n",
        "    #checking to see if we already initialized our model\n",
        "    \n",
        "    #append to layers so we have a correct index value\n",
        "    index = layer_number\n",
        "\n",
        "    #making sure our data in a numpy array\n",
        "    if (type(input_data) == np.ndarray):\n",
        "      X = input_data\n",
        "    else:\n",
        "      X = np.asarray(input_data)\n",
        "\n",
        "    if (self.initialized == False):\n",
        "      #adding data and activations to self lists\n",
        "      self.X.append(X)\n",
        "      self.A.append(activation)\n",
        "\n",
        "      #keep track of our index & initializing random weights for dynamic comatibility testing\n",
        "      self.initial_weights(input_data, output_shape)\n",
        "\n",
        "      X2 = self.forward(input_data, index)\n",
        "      #printing layer info \n",
        "      print(\"Layer:\", index)\n",
        "      print(\"Input Shape: \", X.shape)\n",
        "      print(\"Weight Shape: \", self.W[index].shape)\n",
        "      print(\"Output Shape: \", X2.shape)\n",
        "      print(\" \")\n",
        "      self.layers = self.layers + 1\n",
        "      if (final_layer == True):\n",
        "        self.initialized = True\n",
        "      return(X2)\n",
        "    else:\n",
        "      X2 = self.forward(input_data, index)\n",
        "      return(X2)\n",
        "    \n",
        "    \n",
        "  \n",
        "  def forward(self, input_data, index):\n",
        "    #pulling weights and biases from  main lists for operations\n",
        "    B = self.B[index]\n",
        "    W = self.W[index]\n",
        "\n",
        "    #matmul of data # weights + bias\n",
        "    Z = np.matmul(input_data, W) + B\n",
        "\n",
        "    #pulling activation from index \n",
        "    act = str(self.A[index])\n",
        "    #activating \n",
        "    Z = activate(Z, act)\n",
        "    #keeping track of Z i guess\n",
        "    self.Z.append(Z)\n",
        "    return(Z)\n",
        "\n",
        "  \n",
        "  def backprop(self, model_output):\n",
        "    model_output = model_output\n",
        "    for i in range(len(self.layers)):\n",
        "      i = self.layers - i\n",
        "      act = str(self.A[i])\n",
        "      Z = self.Z[i]\n",
        "      model_output = np.transpose(d_activate(Z, act)) * model_output\n",
        "      self.W[i] = self.W[i] - self.lr*model_output\n",
        "      self.B[i] = self.B[i] - self.lr*model_output\n",
        "      \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h55WSb_oOG5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "ae0eae66-c789-4ee7-8ee7-47a0156291ad"
      },
      "source": [
        "model = Model(128, 0.01)\n",
        "\n",
        "X = np.array([[1,3], [3,2], [3,3]])\n",
        "print(X.shape)\n",
        "Y = np.array([4, 5, 6])\n",
        "print(Y.shape)\n",
        "\n",
        "Z = model.add(X, 2, \"linear\", 0, False)\n",
        "Z = model.add(Z, 4, \"linear\", 1, False)\n",
        "Z = model.add(Z, 5, \"softmax\", 2, True)\n",
        "Z.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2)\n",
            "(3,)\n",
            "Layer: 0\n",
            "Input Shape:  (3, 2)\n",
            "Weight Shape:  (2, 2)\n",
            "Output Shape:  (3, 2)\n",
            " \n",
            "Layer: 1\n",
            "Input Shape:  (3, 2)\n",
            "Weight Shape:  (2, 4)\n",
            "Output Shape:  (3, 4)\n",
            " \n",
            "Layer: 2\n",
            "Input Shape:  (3, 4)\n",
            "Weight Shape:  (4, 5)\n",
            "Output Shape:  (3, 5)\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcPzHpy5Og3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}